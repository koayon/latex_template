@misc{openai_saes_2024,
	title = {Scaling and evaluating sparse autoencoders},
	url = {http://arxiv.org/abs/2406.04093},
	doi = {10.48550/arXiv.2406.04093},
	abstract = {Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Gao, Leo and la Tour, Tom Dupr√© and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04093 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/Kola/Zotero/storage/ZTLUHV6I/Gao et al. - 2024 - Scaling and evaluating sparse autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/Kola/Zotero/storage/DXXUYTGA/2406.html:text/html},
}

@misc{ayonrinde2024_mdl_saes,
  title         = {Interpretability as Compression: Reconsidering SAE Explanations of Neural Activations with MDL-SAEs},
  author        = {Kola Ayonrinde and Michael T. Pearce and Lee Sharkey},
  year          = {2024},
  eprint        = {2410.11179},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2410.11179}
}

@inproceedings{ayonrinde2025bidirectional_interp,
  title={Position: Interpretability is a Bidirectional Communication Problem},
  author={Kola Ayonrinde},
  booktitle={ICLR 2025 Workshop on Bidirectional Human-AI Alignment},
  year={2025},
  url={https://openreview.net/forum?id=O4LaRH4zSI}
}

@misc{ayonrinde2025phil_explanations,
      title={A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i},
      author={Kola Ayonrinde and Louis Jaburi},
      year={2025},
      eprint={2505.00808},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.00808},
}

@misc{ayonrinde2025explanatory_virtues_framework,
      title={Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii},
      author={Kola Ayonrinde and Louis Jaburi},
      year={2025},
      eprint={2505.01372},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.01372},
}
